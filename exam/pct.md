# Параллельные вычислительные технологии

### **1. Архитектура вычислительных систем с распределенной памятью и гибридные системы**  

#### **1) Вычислительные системы с распределенной памятью**  
**Определение:** Это системы, в которых каждый вычислительный узел имеет собственную локальную память, а взаимодействие между узлами осуществляется через коммуникационную сеть.

#### **Конфигурация вычислительных узлов:**  
- **Процессоры:** Многоядерные CPU, графические ускорители (GPU).
- **Память:** Локальная память у каждого узла.
- **Коммуникации:** Используются межсоединения (interconnects), такие как InfiniBand, Ethernet.

#### **Структуры коммуникационных сетей:**  
1. **Полносвязная сеть:** Каждая пара узлов соединена напрямую. Высокая стоимость реализации.  
2. **Кольцо:** Узлы соединены по кольцу; сообщения передаются через промежуточные узлы.  
3. **Шина:** Все узлы подключены к одной общей шине.  
4. **Дерево:** Узлы организованы в иерархическую структуру.  
5. **Гиперкуб:** Узлы соединены в форме гиперкуба, что обеспечивает хорошие показатели производительности.

#### **2) Гибридные вычислительные системы на базе специализированных ускорителей**  
Гибридные вычислительные системы используют комбинацию классических процессоров (CPU) и специализированных ускорителей (GPU, FPGA).  

- **GPU (Graphics Processing Unit):** Высокая производительность в задачах с параллельными вычислениями, таких как машинное обучение и графические расчёты.
- **FPGA (Field-Programmable Gate Array):** Позволяет программировать архитектуру под конкретную задачу, обеспечивает высокую энергоэффективность.
- **TPU (Tensor Processing Unit):** Специализированные ускорители для задач нейронных сетей.

**Пример гибридной системы:** суперкомпьютеры, такие как NVIDIA DGX-2, сочетают GPU и CPU для высокопроизводительных вычислений.

---

### **2. Показатели эффективности параллельных алгоритмов и программ**  

#### **1) Коэффициент ускорения (Speedup)**  

S = T1/Tp
- T1 — время последовательного выполнения программы.
- Tp — время выполнения параллельной программы на \(p\) процессорах.

#### **2) Коэффициент эффективности (Efficiency)**  
E = S/p = T1/(p*Tp)
- Показывает, насколько эффективно используются процессоры.  
- E ~= 1 — высокая эффективность, E < 1 — снижение эффективности из-за накладных расходов.

#### **3) Коэффициент накладных расходов (Overhead)**  

O = p * Tp - T1
- Характеризует дополнительные затраты на параллельное выполнение программы, включая коммуникации и синхронизацию.

#### **Анализ масштабируемости параллельных программ**  

**1) Строгая масштабируемость:**  
Программа показывает стабильное улучшение производительности при увеличении числа процессоров для фиксированного объема данных.  

**2) Слабая масштабируемость:**  
Производительность сохраняется при увеличении числа процессоров и пропорциональном увеличении объема задач.

---

### **3. Масштабируемые программы и законы Амдала и Густафсона-Барсиса**  

#### **1) Масштабируемые программы:**  
Программы, которые могут эффективно использовать увеличенное количество процессоров для повышения производительности.

#### **2) Закон Амдала (Amdahl’s Law)**  
Говорит о том, что ускорение параллельной программы ограничено последовательной частью.  
S(p) = 1 / (f + (1-f)/p)
- f — доля последовательного кода.  
- p — число процессоров.

**Вывод:** При увеличении числа процессоров ускорение асимптотически стремится к 1/f.

##### **Пример:**  
Если последовательная часть программы составляет 10% (\(f = 0.1\)), то даже при бесконечном количестве процессоров максимальное ускорение будет равно 10.

#### **3) Закон Густафсона-Барсиса (Gustafson-Barsis Law)**  
Описывает более оптимистичный сценарий, предполагая увеличение объема задачи при увеличении числа процессоров.

S(p) = p - f * (p-1)

**Вывод:** Закон Густафсона показывает, что параллельные вычисления эффективны при увеличении размера задачи.

---

### **4. Основные понятия многопоточного программирования: взаимные блокировки и «гонка данных». Синхронизация: мьютексы и семафоры**

#### **1) Взаимные блокировки (Deadlock)**  
Ситуация, когда несколько потоков ожидают освобождения ресурсов, которые захвачены друг другом, что приводит к остановке работы программы.  

**Условия возникновения взаимной блокировки:**  
- Взаимное исключение (Mutual exclusion): ресурс может быть захвачен только одним потоком.  
- Удержание и ожидание (Hold and wait): поток удерживает один ресурс и ожидает другой.  
- Отказ от принудительного освобождения (No preemption): ресурсы не могут быть принудительно изъяты.  
- Циклическое ожидание (Circular wait): существует цепочка ожидания потоков.  

**Методы предотвращения:**  
- Наложение порядка захвата ресурсов.  
- Использование таймаутов для ожидания ресурсов.  
- Избегание удержания нескольких ресурсов одновременно.  

#### **2) «Гонка данных» (Race Condition)**  
Ситуация, когда результат выполнения программы зависит от некорректного порядка выполнения потоков.  

**Пример:** Два потока одновременно читают и изменяют общий счётчик без синхронизации.

**Методы предотвращения:**  
- Использование механизмов синхронизации, таких как мьютексы и атомарные операции.

#### **3) Синхронизация: мьютексы и семафоры**  

- **Мьютекс (Mutex, Mutual Exclusion):**  
  Предоставляет взаимное исключение для критических секций кода. Только один поток может захватить мьютекс в каждый момент времени.

  **Основные функции:**  
  - `lock()` — захват мьютекса.  
  - `unlock()` — освобождение мьютекса.  

- **Семафор (Semaphore):**  
  Обеспечивает контроль доступа к ресурсу, который может одновременно использоваться несколькими потоками.  
  Имеет счётчик доступных ресурсов.

  **Основные функции:**  
  - `wait()` — уменьшение счётчика.  
  - `signal()` — увеличение счётчика.  

---

### **5. Основные понятия многопоточного программирования. Атомарные операции**

#### **1) Определение атомарных операций:**  
Операции, которые выполняются полностью (атомарно), без возможности прерывания другими потоками.  

**Примеры атомарных операций:**  
- Инкремент и декремент.  
- Чтение и запись переменной.  

#### **2) Реализация атомарных операций:**  
- В языке C/C++ стандартная библиотека предоставляет заголовок `<atomic>`.  
- Примеры функций: `std::atomic<int> counter`, `fetch_add`, `compare_exchange_strong`.

**Преимущества:**  
- Высокая производительность по сравнению с мьютексами.  
- Минимизация накладных расходов синхронизации.  

**Недостатки:**  
- Ограниченное количество поддерживаемых операций.  

---

### **6. Основные понятия многопоточного программирования. Операция редукции**  

#### **1) Определение:**  
Операция редукции (reduction) — параллельное объединение значений множества элементов в одно значение с использованием ассоциативной операции, такой как сумма, минимум или максимум.

#### **2) Примеры использования:**  
- Сумма элементов массива.  
- Поиск минимального или максимального значения.

#### **3) Реализация редукции:**  
- В OpenMP существует конструкция `reduction`:  
  ```c
  #pragma omp parallel for reduction(+:sum)
  for (int i = 0; i < n; i++) {
      sum += array[i];
  }
  ```

**Преимущества:**  
- Уменьшение количества синхронизаций между потоками.  

---

### **7. Основные понятия многопоточного программирования. Потокобезопасные структуры данных: очереди**

#### **1) Определение потокобезопасных структур данных:**  
Структуры данных, которые обеспечивают корректный доступ из нескольких потоков одновременно без явной синхронизации со стороны программиста.

#### **2) Потокобезопасные очереди:**  
- **Блокирующие очереди (Blocking Queue):** используют мьютексы и условные переменные для синхронизации.  
  Пример: `std::queue` с защитой мьютексом.  
- **Неблокирующие очереди (Lock-Free Queue):** основаны на атомарных операциях и обеспечивают большую производительность.  

#### **3) Пример реализации блокирующей очереди:**  
```cpp
#include <queue>
#include <mutex>
#include <condition_variable>

template<typename T>
class ThreadSafeQueue {
private:
    std::queue<T> queue;
    std::mutex mtx;
    std::condition_variable cv;

public:
    void push(T value) {
        std::lock_guard<std::mutex> lock(mtx);
        queue.push(value);
        cv.notify_one();
    }

    T pop() {
        std::unique_lock<std::mutex> lock(mtx);
        cv.wait(lock, [this] { return !queue.empty(); });
        T value = queue.front();
        queue.pop();
        return value;
    }
};
```
---

### **8. Модель передачи сообщений: стандарт MPI и его реализации. Нумерация процессов и понятие коммуникатора**

#### **1) Стандарт MPI (Message Passing Interface)**  
**MPI** — стандарт программного интерфейса для параллельных вычислений на основе передачи сообщений между процессами. Он используется для разработки высокопроизводительных приложений в вычислительных кластерах и системах с распределенной памятью.

#### **2) Реализации MPI:**  
- **MPICH:** Одна из наиболее распространенных реализаций MPI.  
- **OpenMPI:** Альтернативная и широко поддерживаемая реализация.  
- **Intel MPI:** Оптимизированная версия от Intel для высокопроизводительных систем.  

#### **3) Нумерация процессов и коммуникаторы:**  
**Коммуникатор** — это группа процессов, которые могут обмениваться сообщениями. В MPI используется предопределенный коммуникатор `MPI_COMM_WORLD`, содержащий все процессы программы.

**Основные функции:**  
- `MPI_Comm_size(MPI_COMM_WORLD, &size);` — определяет общее количество процессов.  
- `MPI_Comm_rank(MPI_COMM_WORLD, &rank);` — возвращает номер (ранг) текущего процесса.

**Пример:**  
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    printf("Hello from process %d out of %d\n", rank, size);

    MPI_Finalize();
    return 0;
}
```

---

### **9. Модель передачи сообщений: стандарт MPI и его реализации. Двусторонние обмены стандарта MPI**

#### **1) Двусторонний обмен (point-to-point communication)**  
Передача сообщений между двумя конкретными процессами.

#### **2) Основные функции двустороннего обмена:**  

- `MPI_Send(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)` — отправка сообщения.  
- `MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status)` — прием сообщения.

**Пример передачи сообщений:**  
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    if (rank == 0) {
        int data = 42;
        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
        printf("Process 0 sent data %d\n", data);
    } else if (rank == 1) {
        int data;
        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Process 1 received data %d\n", data);
    }

    MPI_Finalize();
    return 0;
}
```

---

### **10. Модель передачи сообщений: стандарт MPI и его реализации. Коллективные операции обмена информацией**

#### **1) Определение:**  
Коллективные операции обеспечивают обмен данными между всеми или группой процессов.

#### **2) Основные функции коллективных операций:**  

- `MPI_Bcast` — рассылка сообщения от одного процесса ко всем остальным.  
- `MPI_Reduce` — сбор данных и их редукция (например, суммирование).  
- `MPI_Allreduce` — редукция и рассылка результата всем процессам.  
- `MPI_Gather` — сбор данных от всех процессов на одном процессе.  
- `MPI_Scatter` — распределение данных от одного процесса ко всем остальным.

#### **Пример коллективной операции (broadcast):**
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int data = (rank == 0) ? 100 : 0;
    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received data %d\n", rank, data);

    MPI_Finalize();
    return 0;
}
```

---

### **11. Модель передачи сообщений: стандарт MPI и его реализации. Производные типы данных**

#### **1) Определение производных типов данных:**  
Производные (составные) типы данных позволяют передавать сложные структуры, такие как массивы структур, без необходимости их явного разбиения.

#### **2) Создание производных типов данных:**  

- `MPI_Type_contiguous` — создание типа из последовательных элементов.  
- `MPI_Type_vector` — создание типа с регулярными интервалами между элементами.  
- `MPI_Type_struct` — создание типа с произвольными интервалами между элементами.

#### **Пример создания производного типа:**  
```c
#include <mpi.h>
#include <stdio.h>

typedef struct {
    int id;
    double value;
} Data;

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    Data my_data = {1, 3.14};
    MPI_Datatype new_type;
    int block_lengths[2] = {1, 1};
    MPI_Aint displacements[2];
    displacements[0] = offsetof(Data, id);
    displacements[1] = offsetof(Data, value);
    MPI_Datatype types[2] = {MPI_INT, MPI_DOUBLE};

    MPI_Type_create_struct(2, block_lengths, displacements, types, &new_type);
    MPI_Type_commit(&new_type);

    // Использование производного типа для передачи данных
    MPI_Send(&my_data, 1, new_type, 0, 0, MPI_COMM_WORLD);

    MPI_Type_free(&new_type);
    MPI_Finalize();
    return 0;
}
```

---

### **12. Модель передачи сообщений. Подходы к распараллеливанию алгоритмов численного интегрирования**

#### **1) Метод средних прямоугольников:**  

Метод заключается в разбиении области интегрирования на равные отрезки и вычислении суммы произведений длины отрезков на значения функции в серединах этих отрезков. Формула:
\[
I \approx \sum_{i=1}^{N} f\left(x_i\right) \cdot \Delta x
\]

#### **Подход к распараллеливанию:**  
1. Делим диапазон интегрирования на равные подотрезки.  
2. Каждый процесс вычисляет частичную сумму на своем подотрезке.  
3. Суммируем частичные результаты с помощью `MPI_Reduce`.

##### **Пример кода с использованием MPI:**  
```c
#include <mpi.h>
#include <stdio.h>
#include <math.h>

double f(double x) {
    return sin(x);  // Функция для интегрирования
}

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    double a = 0.0, b = M_PI; // Интервал интегрирования [0, π]
    int n = 1000000;          // Число отрезков
    double h = (b - a) / n;

    double local_sum = 0.0;
    int start = rank * (n / size);
    int end = (rank + 1) * (n / size);

    for (int i = start; i < end; i++) {
        double x = a + (i + 0.5) * h;
        local_sum += f(x);
    }

    double total_sum;
    MPI_Reduce(&local_sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        double result = total_sum * h;
        printf("Result: %.10f\n", result);
    }

    MPI_Finalize();
    return 0;
}
```

#### **2) Метод Монте-Карло:**  

Метод Монте-Карло заключается в случайном выборке точек внутри области интегрирования и оценке значения интеграла как среднего значения функции, умноженного на площадь области.

#### **Подход к распараллеливанию:**  
1. Каждый процесс генерирует свои случайные точки и вычисляет частичное среднее.  
2. С помощью `MPI_Reduce` агрегируем результаты.

---

### **13. Модель передачи сообщений. Подходы к распараллеливанию алгоритмов матричных вычислений: умножение матрицы на вектор**

#### **1) Описание задачи:**  
Рассматривается вычисление произведения матрицы `A[N][M]` на вектор `x[M]`:
\[
y_i = \sum_{j=1}^{M} A_{ij} \cdot x_j
\]

#### **Подход к распараллеливанию:**  
1. Разделяем строки матрицы между процессами.  
2. Каждый процесс вычисляет часть результирующего вектора `y`.  
3. С помощью `MPI_Gather` собираем результирующий вектор.

##### **Пример кода:**  
```c
#include <mpi.h>
#include <stdio.h>

#define N 4
#define M 4

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    double A[N][M] = {{1, 2, 3, 4}, {5, 6, 7, 8}, {9, 10, 11, 12}, {13, 14, 15, 16}};
    double x[M] = {1, 1, 1, 1};
    double y[N] = {0};

    int rows_per_process = N / size;
    double local_y[rows_per_process];

    for (int i = 0; i < rows_per_process; i++) {
        int global_row = rank * rows_per_process + i;
        local_y[i] = 0;
        for (int j = 0; j < M; j++) {
            local_y[i] += A[global_row][j] * x[j];
        }
    }

    MPI_Gather(local_y, rows_per_process, MPI_DOUBLE, y, rows_per_process, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Result vector: ");
        for (int i = 0; i < N; i++) {
            printf("%.2f ", y[i]);
        }
        printf("\n");
    }

    MPI_Finalize();
    return 0;
}
```

---

### **14. Модель передачи сообщений. Подходы к распараллеливанию прямых методов решения систем линейных алгебраических уравнений: метод Гаусса**

#### **1) Метод Гаусса:**  
Метод последовательного исключения переменных для решения систем линейных уравнений вида `Ax = b`.

#### **Подход к распараллеливанию:**  
1. Делим строки матрицы между процессами.  
2. На каждом шаге один процесс вычисляет ведущий элемент и рассылает его всем процессам.  
3. Каждый процесс обновляет свои строки.

#### **Основные этапы:**  
- Рассылка ведущего элемента (`MPI_Bcast`).  
- Параллельное обновление строк матрицы (`MPI_Scatter`/`MPI_Gather`).

---

### **15. Модель передачи сообщений. Подходы к распараллеливанию сеточных методов: решение стационарного двумерного уравнения Лапласа**

#### **1) Постановка задачи:**  
Рассматривается решение уравнения Лапласа на прямоугольной сетке:
\[
\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0
\]

#### **Метод решения:**  
Итерационный метод Якоби (Jacobi Method):
1. Разделяем сетку между процессами.
2. Каждый процесс вычисляет свои точки.
3. На каждом шаге процессы обмениваются значениями на границах.

#### **Подход к распараллеливанию:**  
- Обмен значениями граничных точек между соседними процессами (`MPI_Sendrecv`).  
- Итеративное обновление значений на каждом процессе.  

#### **Пример структуры MPI-кода:**  
1. Инициализация MPI и разбиение сетки.  
2. Итерационный процесс обмена граничными значениями.  
3. Проверка критерия остановки и сбор результатов.  
